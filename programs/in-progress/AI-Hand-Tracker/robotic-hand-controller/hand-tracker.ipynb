{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from shared_memory_dict import SharedMemoryDict\n",
    "\n",
    "finger_data_memory = SharedMemoryDict(name='finger-data', size=1024)\n",
    "\n",
    "\n",
    "\n",
    "#import math\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "from threading import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "pTime = 0\n",
    "cTime = 0\n",
    "x_pos = 0\n",
    "y_pos = 0\n",
    "sep = 0\n",
    "loop_counter = 0\n",
    "scale_taken = False\n",
    "clenched_taken = False\n",
    "palm_taken = False\n",
    "finger_data_memory[\"Take-Scale\"] = False\n",
    "\n",
    "finger_positions = np.zeros(shape=(22,3))\n",
    "print(finger_positions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCoordinates:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def middle_point(self, other):\n",
    "        S_O_X_Middle = (self.x + other.x) / 2\n",
    "        S_O_Y_Middle = (self.y + other.y) / 2\n",
    "        return(S_O_X_Middle, S_O_Y_Middle)\n",
    "        \n",
    "    def point_seperation(self, other):\n",
    "        S_O_X_Separation = self.x - other.x\n",
    "        S_O_Y_Separation = self.y - other.y\n",
    "        S_O_Separation = (((S_O_X_Separation ** 2) + (S_O_Y_Separation ** 2))) ** 0.5\n",
    "        return(S_O_Separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_scale(current_sep, reference_sep):\n",
    "    try:\n",
    "        real_sep = current_sep / reference_sep\n",
    "        return(real_sep)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "def scale_variable(value, min_in, max_in, min_out, max_out):\n",
    "    normalized_value = (value - min_in) / (max_in - min_in)\n",
    "    scaled_value = (normalized_value * (max_out - min_out)) + min_out\n",
    "\n",
    "    return scaled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "finger_tops = [4, 8, 12, 16, 20]\n",
    "\n",
    "finger_reference_points = [3, 7, 11, 15, 19]\n",
    "\n",
    "finger_bottoms = [1, 5, 9, 13, 17]\n",
    "#finger_bottoms = [1, 6, 10, 14, 18]\n",
    "finger_seps = [0, 0, 0, 0, 0]\n",
    "\n",
    "finger_seps_initial = [0, 0, 0, 0, 0]\n",
    "\n",
    "finger_seps_raw = [0, 0, 0, 0, 0]\n",
    "\n",
    "calibration_scales = [0, 0, 0, 0, 0]\n",
    "\n",
    "calibration_scales_clenched = [0, 0, 0, 0, 0]\n",
    "\n",
    "current_scales = [0, 0, 0, 0, 0]\n",
    "\n",
    "current_word = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the code, the values from this program need to be written to a CSV file and read simultaneously by another python program as the PC has issues sending through data to the pi as well as doing the motion capture. open file or pandas should do the trick. Also Blue Lobster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liam McCord\\AppData\\Local\\Temp\\ipykernel_16228\\1015987764.py:3: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  real_sep = current_sep / reference_sep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLENCHED: [182.00274723201295, 69.46221994724903, 67.05221845696084, 53.48831648126533, 60.166435825965294]\n",
      "[182.03571078225283, 151.16877984557524, 162.11107303327555, 155.36408851468863, 130.91982279242512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liam McCord\\AppData\\Local\\Temp\\ipykernel_16228\\3529603055.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  sep = (1 / scaling) * sep\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Liam McCord\\Documents\\Code\\Python\\hand-tracking\\robotic-hand-controller\\hand-tracker.ipynb Cell 8\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Liam%20McCord/Documents/Code/Python/hand-tracking/robotic-hand-controller/hand-tracker.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m success, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Liam%20McCord/Documents/Code/Python/hand-tracking/robotic-hand-controller/hand-tracker.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m imgRGB \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Liam%20McCord/Documents/Code/Python/hand-tracking/robotic-hand-controller/hand-tracker.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(imgRGB)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Liam%20McCord/Documents/Code/Python/hand-tracking/robotic-hand-controller/hand-tracker.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m clenched_taken \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Liam%20McCord/Documents/Code/Python/hand-tracking/robotic-hand-controller/hand-tracker.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     finger_data_memory[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m finger_seps\n",
      "File \u001b[1;32mc:\\Users\\Liam McCord\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[1;32mc:\\Users\\Liam McCord\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while True: \n",
    "    success, img = cap.read()\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(imgRGB)\n",
    "    if clenched_taken == True:\n",
    "        finger_data_memory[\"data\"] = finger_seps\n",
    "    \n",
    "    for finger in range(0, len(finger_tops)):\n",
    "        \n",
    "        finger_1 = finger_positions[finger_tops[finger]][0], finger_positions[finger_tops[finger]][1]\n",
    "        finger_2 = finger_positions[finger_bottoms[finger]][0], finger_positions[finger_bottoms[finger]][1]\n",
    "        middle_point = finger_positions[finger_reference_points[finger]][0], finger_positions[finger_reference_points[finger]][1]\n",
    "        \n",
    "        point_1 = PointCoordinates(finger_1[0], finger_1[1])\n",
    "        point_2 = PointCoordinates(finger_2[0], finger_2[1])\n",
    "        \n",
    "        cv2.line(img, (int(point_1.x), int(point_1.y)), (int(point_2.x), int(point_2.y)), (0, 255, 0), thickness=2)\n",
    "        x_pos, y_pos = PointCoordinates.middle_point(point_1, point_2)\n",
    "        sep = PointCoordinates.point_seperation(point_1, point_2)\n",
    "\n",
    "        \n",
    "        point_3 = PointCoordinates(middle_point[0], middle_point[1])\n",
    "        \n",
    "        scaling_sep = PointCoordinates.point_seperation(point_1, point_3)\n",
    "        current_scales[finger] = scaling_sep\n",
    "        initial_sep = finger_seps_initial[finger]\n",
    "        finger_seps_raw[finger] = sep\n",
    "        \n",
    "        cv2.putText(img, current_word, (200, 50), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "        (0, 0, 255), 2)\n",
    "        if scale_taken == True:\n",
    "            scaling = relative_scale(scaling_sep, calibration_scales[finger])\n",
    "            sep = (1 / scaling) * sep\n",
    "            #sep = (sep / initial_sep) * 180\n",
    "            sep = scale_variable(sep, calibration_scales_clenched[finger] + 15, finger_seps_initial[finger], 0, 180)\n",
    "            \n",
    "            if sep < 20:\n",
    "                sep = 0\n",
    "            sep = np.clip(sep, 0, 180)\n",
    "        \n",
    "        cv2.putText(img, f\"({x_pos}, {y_pos}), Sep = ({sep:.2f})\", (int(x_pos), int(y_pos)), cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "        (255, 0, 255), 1) \n",
    "    \n",
    "        finger_seps[finger] = sep\n",
    "        \n",
    "        if sep < 70:\n",
    "            cv2.circle(img, (int(x_pos), int(y_pos)), 15, (255, 0, 0), cv2.FILLED)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if scale_taken == True and palm_taken == True and clenched_taken == False:\n",
    "        if loop_counter - scale_loop_2 == 50:\n",
    "            clenched_taken = True\n",
    "            time.sleep(2)\n",
    "            for n in range(len(calibration_scales_clenched)):\n",
    "                calibration_scales_clenched[n] = finger_seps_raw[n]\n",
    "            print(f\"CLENCHED: {calibration_scales_clenched}\")\n",
    "            print(finger_seps_initial)\n",
    "            current_word = \"\"\n",
    "         \n",
    "    if scale_taken == True and palm_taken == False:\n",
    "        if loop_counter - scale_loop_1 == 50:\n",
    "            scale_loop_2 = loop_counter\n",
    "            palm_taken = True\n",
    "            time.sleep(2)\n",
    "            for n in range(len(calibration_scales)):\n",
    "                calibration_scales[n] = current_scales[n]\n",
    "                finger_seps_initial[n] = finger_seps_raw[n]\n",
    "            current_word = \"Clench your fist.\"\n",
    "            \n",
    "    if scale_taken == False and finger_data_memory[\"Take-Scale\"] == True:\n",
    "        scale_loop_1 = loop_counter\n",
    "        scale_taken = True\n",
    "        current_word = \"Spread your hand.\"\n",
    "          \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            \n",
    "            #print(temp_data_gathering)\n",
    "            for id, lm in enumerate(handLms.landmark):\n",
    "                # Goes through the individual fingers?\n",
    "                \n",
    "                # print(id, lm)\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                  #print(id, cx, cy\n",
    "                  \n",
    "                for point in range(0,22):\n",
    "                    if point == id:\n",
    "                        finger_positions[id][0] = cx\n",
    "                        finger_positions[id][1] = cy\n",
    "            mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime - pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, 3,\n",
    "                (255, 0, 255), 3)\n",
    "    #cv2.putText(img, f\"Ball Coords (x,y,z): ({x_sep}, {int(finger_positions[0][1])}\", (10, 450), cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "    #           (255, 0, 255), 1)\n",
    "    loop_counter += 1\n",
    "    #print(finger_seps)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
